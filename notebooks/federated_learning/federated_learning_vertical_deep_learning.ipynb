{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated learning: deep learning for vertically partitioned data \n",
    "\n",
    "In this notebook, we provide a simple example of how to perform a **vertical** federated learning experiment with the help of the Sherpa.ai Federated Learning framework. \n",
    "As opposed to the horizontal federated learning paradigm, in a vertical federated learning setting (see e.g. [Federated Machine Learning: Concept and Applications](https://arxiv.org/abs/1902.04885)) the different nodes possess the same samples, but different features. \n",
    "A practical example being that of a local on-line shop and an insurance company: both entities might have matching customers (samples), but the information (features) each entity possesses about the customers is of different nature. \n",
    "We are going to use a synthetic dataset and a neural network model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "We use `sklearn` module for generating synthetic databases. \n",
    "Moreover, in order to simulate a vertically partitioned training data, we randomly split the features of the created dataset among the clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from shfl.private.reproducibility import Reproducibility\n",
    "\n",
    "# Comment to turn off reproducibility:\n",
    "Reproducibility(567)\n",
    "\n",
    "# Create dataset\n",
    "n_features = 20\n",
    "n_classes = 2\n",
    "n_samples = 15000\n",
    "\n",
    "data, labels = make_classification(\n",
    "    n_samples=n_samples, n_features=n_features, \n",
    "    n_redundant=0, n_repeated=0, n_classes=n_classes, \n",
    "    n_clusters_per_class=1, flip_y=0.1, class_sep=0.4, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vertical split of the dataset.** In the vertical FL setting, the database is split along the columns (i.e., vertically) among the nodes. \n",
    "We can use a method provided by the Sherpa FL Framework to randomly split a dataset vertically into the desired number of parts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.data_base.data_base import vertical_split\n",
    "\n",
    "# Create a vertically split dataset: split the features among clients\n",
    "M = 2  # number of clients\n",
    "train_data, train_labels, test_data, test_labels = \\\n",
    "    vertical_split(data=data, labels=labels)\n",
    "\n",
    "for item in train_data:\n",
    "    print(\"Client train data shape: \" + str(item.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wrap into NodesFederation.** At this point, we assign the data to a federated network of clients. \n",
    "Since the clients actually don't possess the labels (only the server does), we assign the client's labels to `None`. \n",
    "And since we already performed the split of data for each client, we just need convert it to federated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to federated data: \n",
    "from shfl.private.federated_operation import federate_list\n",
    "\n",
    "nodes_federation = federate_list(train_data)\n",
    "print(nodes_federation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visually check everything went fine with the data assignment, we can configure data access to node (otherwise protected by default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check federated data:\n",
    "from shfl.private.utils import unprotected_query\n",
    "\n",
    "nodes_federation.configure_data_access(unprotected_query);\n",
    "nodes_federation[0].query()\n",
    "nodes_federation[0].query().data.shape\n",
    "#nodes_federation[0].query().label\n",
    "print(nodes_federation[0].query().data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model:\n",
    "\n",
    "**Horizontal Vs Vertical Federated Learning.** Both in the  Federated Government is interpreted as follows: \n",
    " - The Federated Government is intended as a *Coordinator*: it defines and schedules the federated computations, but does not have any other function (no data, no model). It is what a user can customize for the specific case problem.\n",
    " - The Federated Data is composed by nodes that can have multiple functions: train, store data, aggregate, make auxiliary computations, predictions etc.\n",
    " - In particular, the Server is itself a *node* that can interact with the Federated Data: it might aggregate, but might also contain data and train on them\n",
    "\n",
    "In Horizontal FL (see e.g. the [basic concepts notebook](./federated_learning_basic_concepts.ipynb)), all nodes have typically the same model, and the server node has also the aggregation function in its model as an attribute but do not train and does not possess any data.\n",
    "Instead in a Vertical FL architecture, the client nodes might have a *different model* with respect each other and with respect the server node.\n",
    "The latter in turn can aggregate, train and might possess its own data (i.e. the labels in this case).\n",
    "\n",
    "Note that the distinction between client and server is *only virtual* and not necessarily physical, since a single node might be both client and server, allowing multiple roles for the same physical node.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the server node.** We said that in the Vertical FL, each node, including the server, is allowed to possess a different model and different methods for interacting with the clients.\n",
    "We here define the server model with specific functions needed for the present Vertical FL architecture.\n",
    "The server is assigned a linear model, along with the data to train on (only labels, in this specific example): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from shfl.model.vertical_deep_learning_model import VerticalNeuralNetServer\n",
    "from shfl.private.federated_operation import VerticalServerDataNode\n",
    "from shfl.private.data import LabeledData\n",
    "\n",
    "n_embeddings = 2\n",
    "\n",
    "model_server = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n_embeddings, 1, bias=True),\n",
    "    torch.nn.Sigmoid())\n",
    "\n",
    "loss_server = torch.nn.BCELoss(reduction=\"mean\")\n",
    "optimizer_server = torch.optim.SGD(params=model_server.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def roc_auc(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    # Arguments:\n",
    "        y_pred: Predictions \n",
    "        y_true: True labels\n",
    "    \"\"\"\n",
    "    return roc_auc_score(y_true, y_pred)\n",
    "\n",
    "model = VerticalNeuralNetServer(model_server, loss_server, optimizer_server, \n",
    "                                      metrics={\"roc_auc\": roc_auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.federated_aggregator import FedSumAggregator\n",
    "\n",
    "# Create the server node: \n",
    "server_node = VerticalServerDataNode(\n",
    "    nodes_federation=nodes_federation, \n",
    "    model=model,\n",
    "    aggregator=FedSumAggregator(),\n",
    "    data=LabeledData(data=None, label=train_labels.reshape(-1,1).astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_server.parameters():\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define specific data access needed for the Vertical FL round.** The specific Vertical FL architecture requires the computation of the Loss and the exchange of convergence parameters. \n",
    "Namely, the clients send the computed embeddings to the server, and the server sends the computed gradients to update the clients. \n",
    "Therefore, we define ad-hoc access definitions for these methods, and we assign them to server and clients: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_set_evaluation(data, **kwargs): \n",
    "    \"\"\"Evaluate collaborative model on batch train data.\"\"\"\n",
    "    server_model = kwargs.get(\"server_model\")\n",
    "    embeddings, embeddings_indices = kwargs.get(\"meta_params\")\n",
    "    labels = data.label[embeddings_indices]\n",
    "\n",
    "    evaluation = server_model.evaluate(embeddings, labels)\n",
    "\n",
    "    return evaluation\n",
    "    \n",
    "\n",
    "def meta_params_query(model, **kwargs):\n",
    "    \"\"\"Returns embeddings (or their gradients) as computed by the local model.\"\"\"\n",
    "    return model.get_meta_params(**kwargs)\n",
    "\n",
    "    \n",
    "# Configure data access to nodes and server\n",
    "nodes_federation.configure_model_access(meta_params_query)\n",
    "server_node.configure_model_access(meta_params_query)\n",
    "server_node.configure_data_access(train_set_evaluation)   \n",
    "\n",
    "print(nodes_federation[1]._model_access_policy)\n",
    "print(server_node._model_access_policy)\n",
    "print(server_node._private_data_access_policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the federated learning experiment\n",
    "\n",
    "We are almost done: we only need to specify which specific model to use for each client node, and the server node. \n",
    "Namely, the clients will run a neural network model, but of course they will have different input size since they possess different number of features. \n",
    "We first don't use hidden layers for the clients model, resulting in a *linear* model (`layer_dims=None` parameter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.model.vertical_deep_learning_model import VerticalNeuralNetClient\n",
    "\n",
    "model0 = nn.Sequential(\n",
    "    nn.Linear(train_data[0].shape[1], n_embeddings, bias=True),\n",
    ")\n",
    "\n",
    "model1 = nn.Sequential(\n",
    "    nn.Linear(train_data[1].shape[1], n_embeddings, bias=True),\n",
    ")\n",
    "\n",
    "optimizer0 = torch.optim.SGD(params=model0.parameters(), lr=0.001)\n",
    "optimizer1 = torch.optim.SGD(params=model1.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 32\n",
    "model_nodes = [VerticalNeuralNetClient(model=model0, loss=None, optimizer=optimizer0, batch_size=batch_size),\n",
    "               VerticalNeuralNetClient(model=model1, loss=None, optimizer=optimizer1, batch_size=batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model0.parameters():\n",
    "    print(layer)\n",
    "    \n",
    "for layer in model1.parameters():\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch models expect by default input data to be `float`, and if they are in double precision it raises an error. \n",
    "We have two options: either convert the node models just created from the default float to double, or convert the input data to float. \n",
    "If we are not concerned about having double precision, but rather we prefer faster computation, we opt for the second strategy. We apply a federated transformation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_to_float(labeled_data):\n",
    "    if labeled_data.data is not None:\n",
    "        labeled_data.data = labeled_data.data.astype(np.float32)\n",
    "        \n",
    "nodes_federation.apply_data_transformation(cast_to_float);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.federated_government.vertical_federated_government import VerticalFederatedGovernment\n",
    "\n",
    "\n",
    "# Create federated government:\n",
    "federated_government = VerticalFederatedGovernment(model_nodes, \n",
    "                                                   nodes_federation, \n",
    "                                                   server_node=server_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training:\n",
    "federated_government.run_rounds(n_rounds=10001, \n",
    "                                test_data=test_data, \n",
    "                                test_label=test_labels.reshape(-1,1), \n",
    "                                eval_freq=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison to Centralized training.** As reference, we can compare the performance of the collaborative model to the centralized training:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(y_test, y_prediction, save_path=None):\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prediction)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    if save_path is not None: \n",
    "        plt.savefig(save_path, bbox_inches = \"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = federated_government._server.predict_collaborative_model(test_data)\n",
    "plot_roc(test_labels, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model Benchmark on centralized data using sk-learn:\n",
    "\n",
    "centralized_train_data = np.concatenate(train_data, axis=1)\n",
    "centralized_test_data = np.concatenate(test_data, axis=1)\n",
    "\n",
    "clf_linear = LogisticRegression(random_state=123).fit(centralized_train_data, train_labels)\n",
    "\n",
    "y_prediction = clf_linear.predict_proba(centralized_test_data)[:, 1]\n",
    "plot_roc(test_labels, y_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-linear model:**\n",
    "\n",
    "We now add a hidden layer in the clients' neural network model, resulting in a *non-linear* model. \n",
    "Namely, we will use a hidden layer in the clients' models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_neurons = 3\n",
    "\n",
    "model0 = nn.Sequential(\n",
    "    nn.Linear(train_data[0].shape[1], n_hidden_neurons, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(n_hidden_neurons, n_embeddings, bias=True)\n",
    ")\n",
    "\n",
    "model1 = nn.Sequential(\n",
    "    nn.Linear(train_data[1].shape[1], n_hidden_neurons, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(n_hidden_neurons, n_embeddings, bias=True)\n",
    ")\n",
    "\n",
    "optimizer0 = torch.optim.SGD(params=model0.parameters(), lr=0.001)\n",
    "optimizer1 = torch.optim.SGD(params=model1.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 32\n",
    "model_nodes = [VerticalNeuralNetClient(model=model0, loss=None, optimizer=optimizer0, batch_size=batch_size),\n",
    "               VerticalNeuralNetClient(model=model1, loss=None, optimizer=optimizer1, batch_size=batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.federated_government.vertical_federated_government import VerticalFederatedGovernment\n",
    "\n",
    "\n",
    "# Create federated government and run training:\n",
    "federated_government = VerticalFederatedGovernment(model_nodes, \n",
    "                                                   nodes_federation, \n",
    "                                                   server_node=server_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_government.run_rounds(n_rounds=150001, \n",
    "                                test_data=test_data, \n",
    "                                test_label=test_labels.reshape(-1,1), \n",
    "                                eval_freq=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can compare the performance to the analogous centralized model using a hidden layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = federated_government._server.predict_collaborative_model(test_data)\n",
    "plot_roc(test_labels, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-linear benchmark\n",
    "clf_non_linear = MLPClassifier(hidden_layer_sizes=(3,), max_iter=10000, \n",
    "                               shuffle=False, random_state=3221)\n",
    "clf_non_linear.fit(centralized_train_data, train_labels)\n",
    "\n",
    "y_prediction = clf_non_linear.predict_proba(centralized_test_data)[:, 1]\n",
    "plot_roc(test_labels, y_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SherpaFL_py37",
   "language": "python",
   "name": "sherpafl_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
